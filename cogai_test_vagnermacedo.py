# -*- coding: utf-8 -*-
"""Cogai_test_VagnerMacedo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKrLMMCMKppdJI0KfyQKEwQo2exfR-F3
"""

# !pip install pyspark

"""# **# Requisitos**

***1.   Conversão do formato dos arquivos: Converter o arquivo CSV presente no diretório data/input/users/load.csv, para um formato colunar de alta performance de leitura de sua escolha. Justificar brevemente a escolha do formato;***

**Justificativa da escolha do formato PARQUET**

Além das vantagens em utilizar uma representação colunar, o formato Parquet possui as seguintes vantagens/funcionalidades:

*   Estruturas de Dados Complexas
*   Compactação e Enconding
*   Null Values
*   Filter Pushdown
"""

from pyspark.sql import SparkSession
import json
from pyspark.sql.functions import col, rank, max,  dense_rank
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("CognitivoAI").getOrCreate()

# lendo arquivo load.csv da CognitivoAI
df = spark.read.csv("sample_data/load.csv", inferSchema=True, header=True)

#df.show()

# Gravação do Arquivo no formato PARQUET
df.write.format("parquet").save("output_parquet")

# teste para ler o parquet
df_parquet = spark.read.parquet("output_parquet")

#df_parquet.show()

#!ls sample_data -lrt

#!ls -lrt output_parquet/

"""***2.   Deduplicação dos dados convertidos: No conjunto de dados convertidos haverão múltiplas entradas para um mesmo registro, variando apenas os valores de alguns dos campos entre elas. Será necessário realizar um processo de deduplicação destes dados, a fim de apenas manter a última entrada de cada registro, usando como referência o id para identificação dos registros duplicados e a data de atualização (update_date) para definição do registro mais recente;***"""

MyWindow = Window.partitionBy(df['id']).orderBy(df['update_date'].desc())

df2_Window = df.withColumn("rank",rank().over(MyWindow))
#df2_Window.show()

df2_final_rank = df2_Window.filter("rank=1").drop("rank")
#df2_final_rank.show()

# Gravação do Arquivo no formato PARQUET
df2_final_rank.write.format("parquet").save("output_parquet_req2")

"""**3. Conversão do tipo dos dados deduplicados: No diretório config haverá um arquivo JSON de configuração (types_mapping.json), contendo os nomes dos campos e os respectivos tipos desejados de output. Utilizando esse arquivo como input, realizar um processo de conversão dos tipos dos campos descritos, no conjunto de dados deduplicados;**"""

# tipagem atual
#df2_final_rank.dtypes

# usei o arquivo "types_mapping.json" no formato 
with open('sample_data/types_mapping.json') as json_file:  
	    dados = json.load(json_file)
	    for t in dados:
	      df2_final_rank = df2_final_rank.withColumn(t,col(t).cast(dados[t]))

#df2_final_rank.dtypes

# Gravação do Arquivo no formato PARQUET
df2_final_rank.write.format("parquet").save("output_parquet_req3")

"""# Notas gerais
- Todas as operações devem ser realizadas utilizando Spark. O serviço de execução fica a seu critério, podendo utilizar tanto serviços locais como serviços em cloud. Justificar brevemente o serviço escolhido (EMR, Glue, Zeppelin, etc.).

- Cada operação deve ser realizada no dataframe resultante do passo anterior, podendo ser persistido e carregado em diferentes conjuntos de arquivos após cada etapa ou executados em memória e apenas persistido após operação final.

- Você tem liberdade p/ seguir a sequência de execução desejada;

- Solicitamos a transformação de tipos de dados apenas de alguns campos. Os outros ficam a seu critério

- O arquivo ou o conjunto de arquivos finais devem ser compactados e enviados por e-mail.
"""



"""@author: Vagner dos Santos Macedo"""

